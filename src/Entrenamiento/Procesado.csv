Tipo;Celda
Procesado;"for i, n in enumerate(basin_name): 
    for j in range(len(FsimdateJunk1[n])):
        FsimdateJunk2[n].append(str(FsimdateJunk1[n][j][0])+'-'+str(FsimdateJunk1[n][j][1])) 
        FobsdateJunk2[n].append(str(FobsdateJunk1[n][j][0])+'-'+str(FobsdateJunk1[n][j][1]))
for i, n in enumerate(basin_name):
    for j in range(len(FsimdateJunk2[n])):
        Fsimdate[n].append(datetime.datetime.strptime(FsimdateJunk2[n][j],'%Y-%m-%d-%H:%M:%S'))
        Fobsdate[n].append(datetime.datetime.strptime(FobsdateJunk2[n][j],'%Y-%m-%d-%H:%M:%S'))
for i, n in enumerate(basin_name):
    for j in range(len(Junkdate[n])):
        FsimJunkdate[n].append(datetime.datetime.strptime(Junkdate[n][i],'%m/%d/%Y-%H:%M:%S'))"
Procesado;"for i, n in enumerate(basin_name):
    ax = plt.subplot(3, 2, jj)
    fig.subplots_adjust(hspace = 0.3, wspace = 0.3)
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    ax.text(.95,.02, str(n), fontsize=12, horizontalalignment='right', transform=ax.transAxes)
    
    textstr1 = '$R^2 = %.2f$' % r2_sed[n]
    ax.text(0.05, 0.94, textstr1, fontsize=14, transform=ax.transAxes, verticalalignment='top', bbox=props)
    
    if (n == 'Mercer'):     
        plt.xlim([0, 120])
        plt.ylim([0, 120])
        x = range(0, 121)
        y = x
        plt.plot(x, y,linestyle='--', color='grey')
    if (n == 'Thornton'): 
        plt.xlim([0, 300])
        plt.ylim([0, 300])
        # add 1:1 line
        x = range(0, 301)
        y = x
        plt.plot(x, y,linestyle='--', color='grey')
    if (n == 'Issaquah'): 
        ax.set_yscale('log')
        ax.set_xscale('log')
        x = range(0, 9000)
        y = x
        plt.plot(x, y,linestyle='--', color='grey')
        plt.xlabel('obs (mg/l))', fontsize=12., labelpad=5)
    plt.ylabel('sim (mg/l)', fontsize=12., labelpad=5)
    
    ax.scatter(sobs[n], ssim[n], s=15, c='black', alpha=0.8)    
    jj = jj + 1
    ax = plt.subplot(3, 2, jj)
    jj = jj + 1
    nnn = len(sobs[n])
    d = np.linspace(1, nnn, num=nnn)/(nnn+1)
    y  = norm.ppf(d,0,1);
    
    # create the axis ticks
    p  = [0.01, 0.25, 0.5, 0.75, 0.9, 0.99];

    tick  = norm.ppf(p,0,1);
    label = ['0.01','0.25','0.5','0.75','0.9','0.99'];
    sobs[n].sort()
    ssim[n].sort()
    ax.plot(y, ssim[n], 'r+', label='sim')
    ax.plot(y, sobs[n],'k*', label='obs')
    if (n == 'Issaquah'):
        ax.set_yscale('log')
    
    print 'Kolmogorov-Smirnov Test: ', stats.ks_2samp(ssim[n], sobs[n])
    ax.xaxis.set_major_locator(ticker.FixedLocator(tick))
    ax.xaxis.set_major_formatter(ticker.FixedFormatter(label))
    ax.tick_params(axis='x', labelsize=11)
    
    if (i == 0):
        ax.legend(loc='best', numpoints = 1)
    plt.ylabel('conc. (mg/l)', fontsize=12., labelpad=5)
    if (n == 'Issaquah'):
        plt.xlabel('probability', fontsize=12., labelpad=5)
"
Procesado;"for i in range(len(FsimSed)):
    if (FsimSed[i] > 30 or FobsSed[i] > 30):
        f.append(Fsimdate[i])
        c.append(FsimSed[i])
        d.append(FsimSedPost[i]) 
        e.append(FsimSedPre[i])"
Procesado;"for variable in list_variables:
    for categories in variable.getchildren():
        for category in categories.iter(""category""):
            for labels in category.iter(""labels""):
                for element in labels.iter(""text""):
                    if element.attrib['{http://www.w3.org/XML/1998/namespace}lang']=='de-DE':
                        print(variable.attrib['name'],category.attrib['name'], element.text)
                        df = df.append(pd.Series([variable.attrib['id'].replace('_',''),category.attrib['id'],variable.attrib['name'],category.attrib['name'], element.text]),ignore_index=True)"
Procesado;"for categories in list_categories:
    for category in categories.iter(""category""):
        for properties in category.iter(""properties""):
            for prop in properties.iter(""property""):
                try:
                    dfp = dfp.append(pd.Series([category.attrib['id'],prop.attrib['name'],prop.attrib['value']]),ignore_index=True)
                except (NameError, KeyError):
                    pass"
Procesado;" while(i<n1 and j<n2):
        if list1[i]<=list2[j]:
            merged_list.append(list1[i])
            i+=1
        else:
            merged_list.append(list2[j])
            j+=1        
    if(i==n1 and j<n2):
        for x in range(j,n2):
            merged_list.append(list2[x])
    elif(j==n2 and i<n1):
        for y in range(i,n1):
            merged_list.append(list1[y])"
Procesado;" if runNum>initialRun:
        for ii in np.arange(initialRun, runNum):
            [s,e] = parseLog(cursor, str(ii).zfill(3))
            offset += len(s)"
Procesado;"for ii,s in enumerate(start):
        if forceWrite or (ii>storedEvent+offset):
            pickleEvent(dbase, run, s, e[ii], ii+offset, path)"
Procesado;"i = 0
response_data = []
for i in range(100):
    try:
        response_data.append(requests.get(url).json())
        i += 1
        time.sleep(3)
    except ValueError:
        continue"
Procesado;"fpath = path + '/book/'
for json_file in response_data:
    with open(path + 'book_'+ str(datetime.now()) +'.json', 'w') as outfile:
        json.dump(json_file, outfile)
        time.sleep(3)"
Procesado;"book = {}
for json_file in lambda_file(json_file_path):
    with open(json_file) as json_data:
        book.update(json.load(json_data))
    print(book)"
Procesado;" for _, passenger in data.iterrows():
        if ""Sex == 'female'"":
            predictions.append(1)
        elif ""Sex == 'male'"" and 'Age'< 10 :
            predictions.append(1)
        else :
            predictions.append(0)"
Procesado;"for substance in ['Glcxt', 'Ac', 'O2']:
    ax1.plot(s['time'], s['[{}]'.format(substance)], linestyle='-', marker='s', 
             markersize=4,
             color=colors[substance], alpha=0.7, label=substance)
    ax3.plot(s['time'], s['EX_{}'.format(substance)], linestyle='-', marker=None, 
             markersize=4,
             color=colors[substance], alpha=0.7, label=substance)
    ax3.plot(s['time'], s['update_{}'.format(substance)], linestyle='-', marker='s', 
             markersize=4,
             color=colors[substance], alpha=0.7, label=substance)"
Procesado;"  for ax in (ax1, ax2, ax3, ax4):
    ax.set_xlabel('time [h]')
    ax.legend()"
Procesado;" pairs = []
    for keys,group in df[ df['target_id'] != df['user_id'] ].groupby(['user_id','target_id']):
        pairs.append( { 'source':int(keys[0]), 'target':int(keys[1]), 'total_freq':len(group) } )
    "
Procesado;" for this_time in times:
        time_str = datetime.datetime.strftime(this_time,""%Y-%m-%d %H:%M:%S"")
        for keys, group in data[data.time <= this_time].groupby(['user_id','target_id']) :
            df.ix[
               (df['source']==keys[0]) & 
               (df['target']==keys[1]), time_str
               ] = len(group)"
Procesado;"proxdata=proxdata.rename(columns = {'user.id':'user_id','remote.user.id.if.known':'target_id'})
proxdata['time'] = [datetime.datetime.strptime(stamp, ""%Y-%m-%d %H:%M:%S"") for stamp in proxdata['time']]"
Procesado;"final_proxpair_df = sum_by_period(proxpair_df, proxdata, time_array)
proxpair_df.columns = [key.split()[0] for key in proxpair_df.keys()]"
Procesado;frame = frame.append(new_line, ignore_index=True)
Procesado;"calldata=calldata.rename(columns = {'time_stamp':'time'})
comdata = pd.concat([calldata, smsdata])
comdata=comdata.rename(columns = {'dest_user_id_if_known':'target_id'})
comdata = comdata.reset_index()
comdata['time'] = [datetime.datetime.strptime(stamp, ""%Y-%m-%d %H:%M:%S"") for stamp in comdata['time']]"
Procesado;"while current_time < maxtime:
        time_array.append(current_time)
        current_time += jump"
Procesado;"  for epoch in range(training_epochs):
        total_batch = int(mnist.train.num_examples/batch_size)
        c=0.0
        avg_cost=0.0

        for i in range(total_batch):
            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)
            batch_X, batch_Y = mnist.train.next_batch(batch_size)
                _,c=sess.run([optimizer,cost], feed_dict={X:batch_X,Y_:batch_Y,lr: learning_rate})
            avg_cost += c / total_batch"
Procesado;"for m,n in matches:
        if m.distance < 0.7*n.distance:
            good.append(m)

    if len(good) > MIN_MATCH_COUNT:
        return len(good)
    else:
        return 0"
Procesado;"while not max_inlines:
        robot.angular_movement(angle, speed)
        time.sleep(1)
        image = kinect.peek_data()
        new_inlines = get_inlines(image)
        print(""new_ilneles "" , new_inlines, ""inlines: "", inlines)
        time.sleep(1)
        kinect.buffer.clear()
        clear_output(wait=True)
        
        if inlines <= new_inlines:
            inlines = new_inlines
            print(""not max_inlines"")
        else:
            max_inlines = False
            print(""yes"")
            return"
Procesado;" good = []
    for m,n in matches:
        if m.distance < 0.7*n.distance:
            good.append(m)

    if len(good) < MIN_MATCH_COUNT:
        return 0
    
    for dmatch in good:
        print(dmatch)
        point2 = kp2[dmatch[0].queryIdx].pt"
Procesado;"f = open(file_mdd,'r', encoding=""UTF-8"")
filedata = f.read()
f.close()
filecontent=filedata.replace('/Arc 3/2000-02-04','/Arc_3/2000_02_04')
filecontent=filecontent.replace('encoding=""UTF-8""','')
tree = etree.fromstring(filecontent)"
Procesado;"try:
    dfx[:0].to_sql(p_out_sql_lists,engine_dv_bbg, if_exists=p_if_exists, index=False)
except ValueError:
    pass"
Procesado;"for categoryid in list_categoryids:
    dfcat = dfcat.append(pd.Series([categoryid.attrib['value'],categoryid.attrib['name']]),ignore_index=True)
dfcat.columns=['CatValue','CatName']
dfcat.CatValue=dfcat.CatValue.astype(int)"
Procesado;"for lr, reg in sorted(results):
    train_accuracy, val_accuracy = results[(lr, reg)]
    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (
                lr, reg, train_accuracy, val_accuracy)"
Procesado;"for cls, cls_name in enumerate(classes):
    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]
    idxs = np.random.choice(idxs, examples_per_class, replace=False)
    for i, idx in enumerate(idxs):
        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)
        plt.imshow(X_test[idx].astype('uint8'))
        plt.axis('off')
        if i == 0:
            plt.title(cls_name)"
Procesado;"for i in range(20):
    if(i%2!=0):
        Exaprox=-(math.pow(x, i)/math.factorial(i))
    else:
        Exaprox=(math.pow(x, i)/math.factorial(i))
    ant=Val2
    Val2+=Exaprox
    Ever=val-Val2
    Et=(Ever/Val2)*100
    Ea=((Val2-ant)/Val2)*100
    tabla.append([i,Val2,str(Et)+"" %"",str(Ea)+"" %""])"
Procesado;"while 1:
    if(i%2==0):
        cosxaprox=math.pow(x, i)/math.factorial(i)
        ant=serie
        if(band==False):
            serie=serie+cosxaprox
            band=True
        else:
            serie=serie-cosxaprox
            band=False
        Ev=math.fabs(Val-serie)
        Et=(Ev/serie)*100
        Ea=((serie-ant)/serie)*100
        tabla.append([n,serie,str(Et)+"" %"",str(Ea)+"" %""])
        n+=1
    if(math.fabs(Ea)<Es):
        break
    i+=1"
Procesado;"with urlopen(url) as response:
    styles = response.read().decode(""utf8"")"
Procesado;"for sourceid in df1.TargetID:
    likes = graph.get_connections(sourceid, ""likes"")
    like_ids = [like['id'] for like in likes['data']]
    like_names= [like['name'] for like in likes['data']]
    dfx=pd.DataFrame(list(zip(like_ids,like_names)))
    dfx[""TargetID""]=sourceid
    df2=pd.concat([df2,dfx])   "
Procesado;"if 0:
    mod.calc_coll_part_lin_state_eq(simplify=True)
    f = mod.ff ##:
    G = mod.gg ##:
    xx = mod.x ##:
    g1 = st.col_split(G)"
Procesado;"for alpha, beta in [(1, 1), (-1, 1), (1, -1), (-1, -1)]:
    column1 = V1*Matrix([alpha*sqrt(-L12), sqrt(L11)])
    column2 = V2*Matrix([ beta*sqrt(-L22), sqrt(L21)])
    test_matrix = st.col_stack(column1, column2)"
Procesado;"for i in range(0, test_num, batch_size):
    batch = test_comment_data[perm[i:i + batch_size]]
    g_loss = generator.pretrain_step(batch)
    test_loss.append(float(g_loss.data))"
Procesado;"network.add_edge( s0, s1 )
network.add_edge( s1, s5 )
network.add_edge( s2, s3 )
network.add_edge( s2, s4 )
network.add_edge( s3, s5 )
network.add_edge( s5, s6 )
network.add_edge( s5, s7 )
network.add_edge( s4, s7 )"
Procesado;" while not (marker == 'X' or marker == 'O'):
        marker = raw_input('Player 1: Do you want to be X or O?').upper()

    if marker == 'X':
        return ('X', 'O')
    else:
        return ('O', 'X')"
Procesado;"for i in range(1,10):
        if space_check(board, i):
            return False"
Procesado;" while position not in '1 2 3 4 5 6 7 8 9'.split() or not space_check(board, int(position)):
        position = raw_input('Choose your next position: (1-9) ')"
Procesado;"while True:
    theBoard = [' '] * 10
    player1_marker, player2_marker = player_input()
    turn = choose_first()
    print(turn + ' will go first.')
    game_on = True

    while game_on:
        if turn == 'Player 1':
            display_board(theBoard)
            position = player_choice(theBoard)
            place_marker(theBoard, player1_marker, position)

            if win_check(theBoard, player1_marker):
                display_board(theBoard)
                print('Congratulations! You have won the game!')
                game_on = False
            else:
                if full_board_check(theBoard):
                    display_board(theBoard)
                    print('The game is a draw!')
                    break
                else:
                    turn = 'Player 2'

        else:
            display_board(theBoard)
            position = player_choice(theBoard)
            place_marker(theBoard, player2_marker, position)

            if win_check(theBoard, player2_marker):
                display_board(theBoard)
                print('Player 2 has won!')
                game_on = False
            else:
                if full_board_check(theBoard):
                    display_board(theBoard)
                    print('The game is a tie!')
                    break
                else:
                    turn = 'Player 1'

    if not replay():
        break"
Procesado;"for i, n in enumerate(basin_name):
    mainpath[n] = 'D:\\Dropbox\\Python_Scripts\\'+str(basin_name[i])
    simsedfilepath[n] = str(mainpath[n])+ '\\sed_sim_obs.txt'
    simotherfilepath[n] = str(mainpath[n])+'\\coliform_sim_obs.txt'
    simTPfilepath[n] = str(mainpath[n])+'\\TP_sim_obs.txt'"
Procesado;"for i, n in enumerate(basin_name): 
    FsimdateJunk1[n] = np.genfromtxt(simsedfilepath[n], dtype=str, skiprows=0, usecols=[0,1])
    FobsdateJunk1[n] = np.genfromtxt(simsedfilepath[n], dtype=str, skiprows=0, usecols=[3,4])
    FsimSed[n] = np.genfromtxt(simsedfilepath[n], dtype=float, skiprows=0, usecols=[2])
    FsimOther[n] = np.genfromtxt(simotherfilepath[n], dtype=float, skiprows=0, usecols=[2])
    FsimTP[n] = np.genfromtxt(simTPfilepath[n], dtype=float, skiprows=0, usecols=[2])
    FobsSed[n] = np.genfromtxt(simsedfilepath[n], dtype=float, skiprows=0, usecols=[5])
    FobsOther[n] = np.genfromtxt(simotherfilepath[n], dtype=float, skiprows=0, usecols=[5])
    FobsTP[n] = np.genfromtxt(simTPfilepath[n], dtype=float, skiprows=0, usecols=[5])"
Procesado;"for i, n in enumerate(basin_name): 
    for j in range(len(FsimdateJunk1[n])):
        FsimdateJunk2[n].append(str(FsimdateJunk1[n][j][0])+'-'+str(FsimdateJunk1[n][j][1])) 
        FobsdateJunk2[n].append(str(FobsdateJunk1[n][j][0])+'-'+str(FobsdateJunk1[n][j][1]))"
Procesado;"for i, n in enumerate(basin_name):
    gradient, intercept, r_value, p_value, std_err = stats.linregress(FsimSed[n],FobsSed[n])
    r2_sed[n] = r_value**2"
Procesado;"for i, n in enumerate(basin_name):
    MPE_Sed[n] = sum((x-y)/x/float(len(FobsSed[n])) for y,x in zip(FsimSed[n],FobsSed[n]))
    print str(n),"":\n"", ""%0.3f"" % MPE_Sed[n], ""(overall MPE)""
    
    SedObs[n] = pd.DataFrame(FobsSed[n])
    Sed75[n] = SedObs[n].quantile(0.75)
    
    for j in range(len(FobsSed[n])):
        if (FobsSed[n][j] > Sed75[n].values):
            idx = j
            FsimSedQ4[n].append(FsimSed[n][idx])
            FobsSedQ4[n].append(FobsSed[n][idx])
            MPE_SedQ4[n] = sum((x-y)/x/float(len(FobsSedQ4[n])) for y,x in zip(FsimSedQ4[n],FobsSedQ4[n]))"
Procesado;"for epoch in range(training_epochs):
        total_batch = int(mnist.train.num_examples/batch_size)
        c=0.0
        avg_cost=0.0
        # Loop over all batches
        for i in range(total_batch):
            
            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)
            
            batch_X, batch_Y = mnist.train.next_batch(batch_size)
            #print(batch_X.shape)
            _,c=sess.run([optimizer,cost], feed_dict={X:batch_X,Y_:batch_Y,lr: learning_rate,pkeep: 0.75})
            avg_cost += c / total_batch"
Procesado;"for i, presplit in enumerate(df[column].astype(str)):
        values = presplit.split(sep)
        if keep and len(values) > 1:
            indexes.append(i)
            new_values.append(presplit)
        for value in values:
            indexes.append(i)
            new_values.append(value)"
Procesado;"for TableName in Parent_L1_Tables.values.tolist():
    dict_levels=dict(zip(levels.DSCTableName,levels.TableName))
    TableName.replace(TableName,dict_levels[TableName])
    con = sqlite3.connect(file_ddf)
    Table=TableName.replace(TableName,dict_levels[TableName])
    df = pd.read_sql_query(""SELECT [Respondent.Serial:L] as Respondent, ""+Table+"".* from ""+Table+""  JOIN L1 ON L1.[:P0]=""+Table+"".[:P1] "", con)
    con.close()
 
    print ([Table]+df.columns.tolist())
    dfnewline=pd.DataFrame([Table]+df.columns.tolist()).T
    dfTables=pd.concat([dfTables,dfnewline])
    dfs=df[df.columns[df.columns.str.contains(':S$|Respondent|LevelId')]].copy()
    dfc1=df[df.columns[df.columns.str.contains(':C1$|Respondent|:P[12]')]].copy()
    if 'ban:S' in dfs.columns:
        dfs=dfs.drop('ban:S', axis=1)

    dict_c1=dict(zip(dfc1.columns,dfc1.columns.str.replace('(:C1|\.Serial:L)','')))

    dfc1.rename(columns=dict_c1, inplace=True)
    dfc1['Level']=TableName
    dfc1m=pd.melt(dfc1,id_vars=['Respondent','Level','LevelId',':P1'],var_name='Variable', value_name='Value')
    dfc1m['Type']='Single'

    dict_s=dict(zip(dfs.columns,dfs.columns.str.replace('(:C1|:S|\.Serial:L)','')))
    dfs.rename(columns=dict_s, inplace=True)
    dfs.loc[:,'Level']=TableName
    dfsm=pd.melt(dfs,id_vars=['Respondent','Level','LevelId'],var_name='Variable', value_name='Value')
    dfsmt=pd.DataFrame([])
    if dfsm.Respondent.count()>0:
        dfsmt=dfsm.drop(['Value'], axis=1).join(dfsm['Value'].str.split(';',expand=True).stack().reset_index(drop=True, level=1).rename('Value') )
        dfsmt=dfsmt[dfsmt.Value>' ']
        dfsmt['Value']=dfsmt['Value'].astype(int)
        dfsmt['Respondent']=dfsmt['Respondent'].astype(int)
        dfsmt['LevelId']=dfsmt['LevelId'].astype(int)
        dfsmt['Type']='Multi'

    dfL1=pd.concat([dfL1,dfc1m])
    dfL1=pd.concat([dfL1,dfsmt])"
NoProcesado;"fig = plt.figure()
ax = plt.axes()
plt.grid()
plt.show()"
NoProcesado;"plt.plot(alphadeg, e)
plt.title('One-dimensional approximation error.')
plt.xlabel('$\\alpha\>\>[^\circ]$', fontsize=14)
plt.ylabel(r'$|e|\>\>[\%]$', fontsize=14)
plt.savefig('tex/figs/q1.pdf')
plt.show()"
NoProcesado;"import matplotlib.pyplot as plt
%matplotlib inline  

nx.draw_networkx(G, width=1, node_size = 100, with_labels=False, pos=nx.fruchterman_reingold_layout(G))#random_layout(G))# fruchterman_reingold_layout(G))
plt.show()
nx.draw_networkx(G, width=1, node_size = 100, with_labels=False, pos=nx.random_layout(G))#random_layout(G))
plt.show()"
NoProcesado;"fig, ax = plt.subplots(2,2, figsize=(12, 7))
fig.suptitle(""Result for $q_l$"", size=16)
ax[0, 0].hist(actual_test_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[0, 0].set_title('Testing actual values')
ax[0, 1].hist(modeled_test_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[0, 1].set_title('Testing modeled values')

ax[1, 0].hist(actual_training_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[1, 0].set_title('Training actual values')

ax[1, 1].hist(modeled_training_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[1, 1].set_title('Training modeled values')
plt.show()"
NoProcesado;"melted = pd.melt(df, id_vars=[""day""], var_name='city', value_name='temperature')
melted"
NoProcesado;"ex_idx = df.index.str.contains('^EX_')
df[ex_idx]"
NoProcesado;"for idx, diff in most_improved[:3000]:
    source_len = len(source_lines[idx].split())
    target_len = len(ref_lines[idx].split())
    edit_distance = get_editdistance(baseline_hyp_lines[idx], ref_lines[idx])     print(edit_distance)
    if (target_len > 10
    and target_len < 30
    and len(constraints[idx]) > 1
    and edit_distance >= 0.5):
        print(u'S1: {} S2: {}'.format(baseline_scores[idx], constrained_scores[idx]))
        print(u'Source: {}'.format(source_lines[idx]))
        print(u'Hyp1: {}'.format(baseline_hyp_lines[idx]))
        print(u'Constraints: {}'.format(constraints[idx]))
        print(u'Hyp2: {}'.format(constrained_hyp_lines[idx]))
        print(u'Ref: {}\n'.format(ref_lines[idx]))"
NoProcesado;"lucky_numbers = [1, 2, 3, 5, 90]
names = [""Michael"", ""Freddy"", ""Jason""]
activated = [True, True, False, False, True]"
NoProcesado;len(lucky_numbers)
NoProcesado;df.plot()
NoProcesado;df.describe()
NoProcesado;"df['Gendercolor'] = df['Gender'].map({'Male': 'blue', 'Female': 'red'})
df.head()"
NoProcesado;"print(""labels.txt \t : \t reviews.txt\n"")
pretty_print_review_and_label(2137)
pretty_print_review_and_label(12816)
pretty_print_review_and_label(6267)
pretty_print_review_and_label(21934)
pretty_print_review_and_label(5297)
pretty_print_review_and_label(4998)"
NoProcesado;"print(""Pos-to-neg ratio for 'the' = {}"".format(pos_neg_ratios[""the""]))
print(""Pos-to-neg ratio for 'amazing' = {}"".format(pos_neg_ratios[""amazing""]))
print(""Pos-to-neg ratio for 'terrible' = {}"".format(pos_neg_ratios[""terrible""]))"
NoProcesado;"hist, edges = np.histogram(list(map(lambda x:x[1],pos_neg_ratios.most_common())), density=True, bins=100, normed=True)

p = figure(tools=""pan,wheel_zoom,reset,save"",
           toolbar_location=""above"",
           title=""Word Positive/Negative Affinity Distribution"")
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=""#555555"")
show(p)"
NoProcesado;Image(filename='sentiment_network_sparse.png')
NoProcesado;"for line in p.stdout.readlines():
    print (line),
retval = p.wait()"
NoProcesado;simres.groupby('Choice_Id')['Accuracy'].mean()
NoProcesado;simres.groupby(['Choice_Id','Mode_Prediction']).size()
NoProcesado;"scatter(X,y)
plot_separator(logreg.predict)"
NoProcesado;"sxtrain2 = sxtrain1[:1049,:,:,:]
sytrain2 = sytrain1[:1049,:,:]
sxtest2 = sxtrain1[1049:,:,:,:]
sytest2 = sytrain1[1049:,:,:]
print(sxtrain2.shape)
print(sxtrain2.dtype)
print(sytrain2.shape)
print(sytrain2.dtype)
print(sxtest2.shape)
print(sxtest2.dtype)
print(sytest2.shape)
print(sytest2.dtype)"
NoProcesado;sytrain2.dtype
NoProcesado;model2.summary()
NoProcesado;"print(preds[0])
ans1 = imresize(preds[0],(480,640))
print(ans1.shape)
plt.imshow(ans1)
plt.show()"
NoProcesado;"print(len(f1.layers))
temp12 = f1.get_layer(""convolution2d_1"")
print(f1.layers[24].output_shape)
print(f1.layers[26].output_shape)
print(f1.layers[27].output_shape)
print(f1.layers[29].output_shape)
print(f1.layers[30].output_shape)
print(f1.layers[27].output)
c1 = f1.get_layer(""lambda_2"")
print(c1.output_shape)
print(f1.layers[25].input_shape)
"
NoProcesado;"path_mdd=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking'.replace(chr(92),'/')
file_mdd=path_mdd+'/'+'data.mdd'

pfad_out=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking\out'.replace(chr(92),'/')
file_cat_out_csv=pfad_out+'/'+p_out_sql_cat+'.csv'
file_id_out_csv=pfad_out+'/'+p_out_sql_id+'.csv'
file_answer_out_csv=pfad_out+'/'+p_out_sql_answer+'.csv'
file_lists_out_csv=pfad_out+'/'+p_out_sql_lists+'.csv'


path_bat=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking\out'+chr(92)
file_bat=path_bat+'/'+'bbg_csv.bat'"
NoProcesado;"positive_counts = Counter()
negative_counts = Counter()
total_counts = Counter()"
NoProcesado;"simres['Accuracy'] = simres.apply(count_correct,axis=1)
simres['Mode_Prediction'] = simres.apply(simul_mode,axis=1)"
NoProcesado;"data = np.load(""newdataset1.npz"")
sxtrain1 = data[""xtrain""]
sytrain1 = data[""ytrain""]"
NoProcesado;"start1 = Flatten()(outtemp1)
start1 = Dense(5120, activation='relu', init='glorot_normal', W_regularizer=l2(0.02))(start1)
start1 = Dropout(0.5)(start1)
start1 = Dense(4389, activation='relu', init='glorot_normal', W_regularizer=l2(0.02))(start1)
start1 = Reshape((57,77),input_shape=(4389,),name='outputFC')(start1)
model2 = Model(input=temp1.layers[0].input, output=start1)"
NoProcesado;"x1 = tf.Variable(name=""x1"", dtype=""float32"", initial_value=np.random.rand(), trainable=True)
x1.initializer.run()
x2 = tf.Variable(name=""x2"", dtype=""float32"", initial_value=np.random.rand(), trainable=True)
x2.initializer.run()"
NoProcesado;"lr = 0.001

opt1 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost1)
opt2 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost2)
opt3 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost3)
opt4 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost4)
opt5 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost5)
opt  = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(globalCost)"
NoProcesado;"r = te.loads(mbounds)
r.timeCourseSelections += r.getGlobalParameterIds()
s = r.simulate(0, 10, steps=200)"
NoProcesado;"mpl.rcParams['lines.linewidth'] = 2
font = {'family' : 'sans-serif',
        'sans-serif' : 'Verdana',
        'weight' : 'medium',
        'size'   : '12'}
params1 = {
          'axes.labelsize': 12,
          'text.fontsize': 12,
          'xtick.labelsize': 12,
          'xtick.direction': 'out',
          'ytick.labelsize': 12,
          'legend.pad': 0.01,     # empty space around the legend box
          'legend.fontsize': 12,
          'legend.labelspacing':0.25,
          'font.size': 12,
          'font.style': 'normal',
          'axes.style': 'normal',
          'xtick.labelstyle': 'normal',
          }
mpl.RcParams.update(params1)
mpl.rc('font', **font)
plt.rc(""xtick"", direction=""out"")
plt.rc(""ytick"", direction=""out"")
plt.rc('legend',**{'fontsize':12})"
NoProcesado;"p_pfad=r'O:\Work\DV\MatthiasH\BBG_Tracking'.replace(chr(92),'/')
p_file='R315114383A.mdd'
path = p_pfad+'/'+p_file"
NoProcesado;"key1 = os.environ.get('API_TRIAL')
url = ""https://api.nytimes.com/svc/books/v3/lists/overview.json?q=new+york+times&sort=newest&api-key=""+(key1)"
NoProcesado;"outcomes = full_data['Survived']
data = full_data.drop('Survived', axis = 1)"
NoProcesado;frame = pd.DataFrame({'numbers':range(10), 'chars':['a']*10})
NoProcesado;"x=mnist.train.images
y=mnist.train.labels"
NoProcesado;"Y1 = (tf.matmul(X, W1))
BN1=batchnorm(Y1,O1,S1)
l1_BN = tf.nn.sigmoid(BN1)

Y2 = (tf.matmul(l1_BN, W2))
BN2=batchnorm(Y2,O2,S2)
l2_BN = tf.nn.sigmoid(BN2)

Y3 = (tf.matmul(l2_BN, W3))
BN3=batchnorm(Y3,O3,S3)
l3_BN = tf.nn.sigmoid(BN3)

Y4 = (tf.matmul(l3_BN, W4))
BN4=batchnorm(Y4,O4,S4)
l4_BN = tf.nn.sigmoid(BN4)

Ylogits_BN = tf.matmul(l4_BN, W5) + B5
Ylogits_BN = tf.nn.softmax(Ylogits_BN)"
NoProcesado;"x=0.3*math.pi
tabla=[]
Es=(0.5*math.pow(10,(-6)))
Val=round(math.cos(x), 8)
serie,i ,n =0,0,1
band=False"
NoProcesado;"url='https://github.com/debimax/cours-debimax/raw/master/documents/custom.css'
with urlopen(url) as response:
    styles = response.read().decode(""utf8"")"
NoProcesado;"token = mytoken
idnow=""654057261334143""

graph = facebook.GraphAPI(token)
profile = graph.get_object(id=idnow)
likes = graph.get_connections(idnow, ""likes"")

like_ids = [like['id'] for like in likes['data']]
like_names= [like['name'] for like in likes['data']]
df1=pd.DataFrame(list(zip(like_ids,like_names)))
df1.columns=(""TargetID"",""TargetName"")
df1[""SourceID""]=idnow
df1[""SourceName""]=""Bundeswehr""
df1"
NoProcesado;"x={
  ""data"": [
    {
      ""name"": ""Bundeswehr"",
      ""id"": ""122840837780517""
    },
    {
      ""name"": ""Bundeswehr"",
      ""id"": ""518147268231689""
    },
    {
      ""name"": ""Bundeswehr"",
      ""id"": ""116477405044872""
    },
    {
      ""name"": ""Bundeswehr Wir.Dienen.Deutschland"",
      ""id"": ""214713535365449""
    },
    {
      ""name"": ""Bundeswehr Karriere"",
      ""id"": ""215977868441680""
    },
    {
      ""name"": ""Die Bundeswehr in Bayern"",
      ""id"": ""1520018021643993""
    },
    {
      ""name"": ""Die Bundeswehr in Schleswig-Holstein"",
      ""id"": ""585997238220800""
    },
    {
      ""name"": ""Deutscher BundeswehrVerband (DBwV)"",
      ""id"": ""155972661134170""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Hamburg"",
      ""id"": ""108522982545311""
    },
    {
      ""name"": ""Milit�rhistorisches Museum der Bundeswehr - MHM Dresden"",
      ""id"": ""220212508031021""
    },
    {
      ""name"": ""F�hrungsakademie der Bundeswehr"",
      ""id"": ""107782152578369""
    },
    {
      ""name"": ""Die Bundeswehr in Mecklenburg-Vorpommern"",
      ""id"": ""571802776358319""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Berlin"",
      ""id"": ""162431180449740""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Ulm"",
      ""id"": ""114199045306653""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Westerstede"",
      ""id"": ""108769105852886""
    },
    {
      ""name"": ""Info/Bundeswehr 2.0"",
      ""id"": ""580108145380270""
    },
    {
      ""name"": ""Die Bundeswehr in Th�ringen"",
      ""id"": ""153648541731952""
    },
    {
      ""name"": ""Bundeswehr und Freizeitshop"",
      ""id"": ""111189415577035""
    },
    {
      ""name"": ""Bundeswehrzentralkrankenhaus Koblenz"",
      ""id"": ""164514060226583""
    },
    {
      ""name"": ""Bundeswehrsteuererkl�rung"",
      ""id"": ""1109333195781095""
    },
    {
      ""name"": ""Universit�t der Bundeswehr M�nchen"",
      ""id"": ""167548988935""
    },
    {
      ""name"": ""Die Bundeswehr in Hamburg"",
      ""id"": ""898884626873778""
    },
    {
      ""name"": ""Schule f�r Feldj�ger und Stabsdienst der Bundeswehr Emmich-Cambrai-Kaserne"",
      ""id"": ""126011754146247""
    },
    {
      ""name"": ""Bw-K - Bundeswehr-Kameradschaft"",
      ""id"": ""160772063980987""
    },
    {
      ""name"": ""Milit�rhistorisches Museum der Bundeswehr - Flugplatz Berlin Gatow"",
      ""id"": ""141375795960016""
    }
  ],
  ""paging"": {
    ""cursors"": {
      ""before"": ""MAZDZD"",
      ""after"": ""MjQZD""
    },
    ""next"": ""https://graph.facebook.com/v2.9/search?access_token=EAACEdEose0cBANYkCCTTCeaPD60yfW8Rw0RQC0oIJJMhaetDlkQyz5ZCL4DHR1040AZBJDwLf57m6fThoRUlyMZAC8ecU9k9OOX9RAIlwCpxVoaL4TIZAaEYV9qBPyZArxRjOIxuokhux3FhMXKG166s0dfsnsQn9UGHIZAER0I1naBqZAUYMvdYy26Ahzmorq6qXBFJoNmlQZDZD&pretty=0&q=Bundeswehr&type=page&limit=25&after=MjQZD""
  }
}"
NoProcesado;"
L11, L12 = st.sorted_eigenvalues(h1)
V1 = st.sorted_eigenvector_matrix(h1)

L21, L22 = st.sorted_eigenvalues(h2)
V2 = st.sorted_eigenvector_matrix(h2)"
NoProcesado;"acTransList = [""SB10001,1000"", ""SB10002,1200"", ""SB10003,8000"", ""SB10004,400"", ""SB10005,300"", ""SB10006,10000"", ""SB10007,500"", ""SB10008,56"", ""SB10009,30"",""SB10010,7000"", ""CR10001,7000"", ""SB10002,-10""]

acTransRDD = sc.parallelize(acTransList)
goodTransRecords = acTransRDD.filter(lambda trans: Decimal(trans.split("","")[1]) > 0).filter(lambda trans: (trans.split("","")[0]).startswith('SB') == True)    
highValueTransRecords = goodTransRecords.filter(lambda trans: Decimal(trans.split("","")[1]) > 1000)
badAmountLambda = lambda trans: Decimal(trans.split("","")[1]) <= 0
badAcNoLambda = lambda trans: (trans.split("","")[0]).startswith('SB') == False
badAmountRecords = acTransRDD.filter(badAmountLambda)
badAccountRecords = acTransRDD.filter(badAcNoLambda)
badTransRecords  = badAmountRecords.union(badAccountRecords)"
NoProcesado;"f = c['/0/data']
temp = f.get_data()
temp = np.array(temp)
temp2 = temp.reshape((512,512))
temp3 = temp2 * 1e9
temp3 = temp3
temp2.shape"
NoProcesado;"high = 255
low = 0
amin = temp2.min()
amax = temp2.max()
rng = amax - amin
test = high - (((high - low) * (amax - temp2)) / rng)"
NoProcesado;"basin_name = ['Mercer', 'Thornton', 'Issaquah']
basin_nameL = ['mercer', 'thornton', 'issaquah']
mainpath = {}
simsedfilepath = {}
simotherfilepath = {}
simTPfilepath = {}

outputfilepath = 'D:\\Box Sync\\WQ-PAPER\\Figures\\'    "
