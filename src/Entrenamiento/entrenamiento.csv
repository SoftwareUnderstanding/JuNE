tipo;celda
Configuracion;"p_out_sql_cat='Cat'
p_out_sql_id='VarID'
p_out_sql_answer='AnswerLabel'
p_out_sql_lists='Lists'"
Configuracion;"path_mdd=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking'.replace(chr(92),'/')
file_mdd=path_mdd+'/'+'data.mdd'
pfad_out=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking\out'.replace(chr(92),'/')
file_cat_out_csv=pfad_out+'/'+p_out_sql_cat+'.csv'
file_id_out_csv=pfad_out+'/'+p_out_sql_id+'.csv'
file_answer_out_csv=pfad_out+'/'+p_out_sql_answer+'.csv'
file_lists_out_csv=pfad_out+'/'+p_out_sql_lists+'.csv'
path_bat=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking\out'+chr(92)
file_bat=path_bat+'/'+'bbg_csv.bat'"
Configuracion;"engine_dv_bbg = create_engine('mssql+pyodbc://TSMMHSQVS901/DV_BBG?driver=SQL+Server+Native+Client+11.0?trusted_connection=yes')
p_if_exists='fail'
if p_mode=='r':
    p_if_exists='replace'
if p_mode=='a':
    p_if_exists='append'"
Configuracion;"positive_counts = Counter()
negative_counts = Counter()
total_counts = Counter()"
Configuracion;"review = ""The movie was excellent""
Image(filename='sentiment_network_pos.png')"
Configuracion;"simres['Accuracy'] = simres.apply(count_correct,axis=1)
simres['Mode_Prediction'] = simres.apply(simul_mode,axis=1)"
Configuracion;"X0,y0 = make_blobs(n_samples=100,centers=[[6,6]])
X1,y1 = make_blobs(n_samples=100,centers=[[7,7]])
y1 += 1
X = np.vstack((X0,X1))
y = np.hstack((y0,y1))"
Configuracion;"data = np.load(""newdataset1.npz"")
sxtrain1 = data[""xtrain""]
sytrain1 = data[""ytrain""]"
Configuracion;"xt1 = sxtrain2[0]
cimg1 = np.zeros((240,320,3),dtype=np.uint8)
cimg1[:,:,0] = xt1[0,:,:]
cimg1[:,:,1] = xt1[1,:,:]
cimg1[:,:,2] = xt1[2,:,:]"
Configuracion;"start1 = Flatten()(outtemp1)
start1 = Dense(5120, activation='relu', init='glorot_normal', W_regularizer=l2(0.02))(start1)
start1 = Dropout(0.5)(start1)
start1 = Dense(4389, activation='relu', init='glorot_normal', W_regularizer=l2(0.02))(start1)
start1 = Reshape((57,77),input_shape=(4389,),name='outputFC')(start1)
model2 = Model(input=temp1.layers[0].input, output=start1)"
Configuracion;"plt.rcParams['image.cmap'] = 'hot'
plt.rcParams['image.interpolation'] = 'none'
fig = plt.figure()
fig.set_figheight(9)
fig.set_figwidth(9)
ind = 0
flag = 0
imageinds = [205,55,228,120,2]"
Configuracion;"x1 = tf.Variable(name=""x1"", dtype=""float32"", initial_value=np.random.rand(), trainable=True)
x1.initializer.run()
x2 = tf.Variable(name=""x2"", dtype=""float32"", initial_value=np.random.rand(), trainable=True)
x2.initializer.run()"
Configuracion;"S1 = 10
S2 = 15
y1 = S1 * x1 + S2 * x2
cost1 = -y1"
Configuracion;"lr = 0.001
opt1 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost1)
opt2 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost2)
opt3 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost3)
opt4 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost4)
opt5 = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost5)
opt  = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(globalCost)"
Configuracion;"plt.rcParams.update({
        'axes.labelsize': 'large', 
        'axes.labelweight': 'bold',
        'axes.titlesize': 'large',
        'axes.titleweight': 'bold',
        'legend.fontsize': 'small',
        'xtick.labelsize': 'large',
        'ytick.labelsize': 'large',
    })"
Configuracion;"r = te.loads(mbounds)
r.timeCourseSelections += r.getGlobalParameterIds()
s = r.simulate(0, 10, steps=200)"
Configuracion;%pylab inline
Configuracion;"mpl.rcParams['lines.linewidth'] = 2
font = {'family' : 'sans-serif',
        'sans-serif' : 'Verdana',
        'weight' : 'medium',
        'size'   : '12'}
params1 = {
          'axes.labelsize': 12,
          'text.fontsize': 12,
          'xtick.labelsize': 12,
          'xtick.direction': 'out',
          'ytick.labelsize': 12,
          'legend.pad': 0.01,     # empty space around the legend box
          'legend.fontsize': 12,
          'legend.labelspacing':0.25,
          'font.size': 12,
          'font.style': 'normal',
          'axes.style': 'normal',
          'xtick.labelstyle': 'normal',
          }
mpl.RcParams.update(params1)
mpl.rc('font', **font)
plt.rc(""xtick"", direction=""out"")
plt.rc(""ytick"", direction=""out"")
plt.rc('legend',**{'fontsize':12})"
Configuracion;"min_date = adj_price.argmin()
max_date = adj_price.argmax()
max_growth_per_year = total_max_growth ** (1.0 / (max_date.year - min_date.year))
max_growth_per_year"
Configuracion;"p_pfad=r'O:\Work\DV\MatthiasH\BBG_Tracking'.replace(chr(92),'/')
p_file='R315114383A.mdd'
path = p_pfad+'/'+p_file"
Configuracion;"list_variables=tree.findall('.//definition/variable')
list_categories=tree.findall('.//definition/categories')
list_fields=tree.findall('.//design/fields')"
Configuracion;"key1 = os.environ.get('API_TRIAL')
url = ""https://api.nytimes.com/svc/books/v3/lists/overview.json?q=new+york+times&sort=newest&api-key=""+(key1)"
Configuracion;"json_file_path = fpath +'/*'
lambda_file = lambda json_file_path : glob.glob(json_file_path)"
Configuracion;"outcomes = full_data['Survived']
data = full_data.drop('Survived', axis = 1)"
Configuracion;"smsdata.columns = [""user_id"",""time"",""incoming"",""dest_user_id_if_known"",""dest_phone_hash""]
smsdata['type'] = ['sms']*len(smsdata) # this repeats string 'sms' len(smsdata) times
calldata['type'] = ['call']*len(calldata)
calldata=calldata.rename(columns = {'time_stamp':'time'})
comdata = pd.concat([calldata, smsdata])
comdata=comdata.rename(columns = {'dest_user_id_if_known':'target_id'})
comdata = comdata.reset_index()"
Configuracion;frame = pd.DataFrame({'numbers':range(10), 'chars':['a']*10})
Configuracion;new_line = {'Name':'Perov', 'Birth':'22.03.1990', 'City':'Penza'}
Configuracion;"x=mnist.train.images
y=mnist.train.labels"
Configuracion;"S1 = tf.Variable(tf.ones([L]))
O1 = tf.Variable(tf.zeros([L]))
S2 = tf.Variable(tf.ones([M]))
O2 = tf.Variable(tf.zeros([M]))
S3 = tf.Variable(tf.ones([N]))
O3 = tf.Variable(tf.zeros([N]))
S4 = tf.Variable(tf.ones([O]))
O4 = tf.Variable(tf.zeros([O]))"
Configuracion;"Y1 = (tf.matmul(X, W1))
BN1=batchnorm(Y1,O1,S1)
l1_BN = tf.nn.sigmoid(BN1)
Y2 = (tf.matmul(l1_BN, W2))
BN2=batchnorm(Y2,O2,S2)
l2_BN = tf.nn.sigmoid(BN2)
Y3 = (tf.matmul(l2_BN, W3))
BN3=batchnorm(Y3,O3,S3)
l3_BN = tf.nn.sigmoid(BN3)
Y4 = (tf.matmul(l3_BN, W4))
BN4=batchnorm(Y4,O4,S4)
l4_BN = tf.nn.sigmoid(BN4)
Ylogits_BN = tf.matmul(l4_BN, W5) + B5
Ylogits_BN = tf.nn.softmax(Ylogits_BN)"
Configuracion;"path_mdd=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking'.replace(chr(92),'/')
file_mdd=path_mdd+'/'+'data.mdd'
pfad_out=r'\\TSHAMFIL901\Work\DV\MatthiasH\BBG_Tracking\out'.replace(chr(92),'/')
file_cat_out_csv=pfad_out+'/'+p_out_sql_cat+'.csv'
file_id_out_csv=pfad_out+'/'+p_out_sql_id+'.csv'
file_answer_out_csv=pfad_out+'/'+p_out_sql_answer+'.csv'
file_lists_out_csv=pfad_out+'/'+p_out_sql_lists+'.csv'
path_bat=r'O:\Work\DV\MatthiasH\BBG_Tracking\out'+chr(92)
file_bat=path_bat+'/'+'bbg.bat'"
Configuracion;"x=0.3*math.pi
tabla=[]
Es=(0.5*math.pow(10,(-6)))
Val=round(math.cos(x), 8)
serie,i ,n =0,0,1
band=False"
Configuracion;"tabla=[]
x=Symbol(""x"")
x1real=5000
x2real=0.002
a,b,c=1,-5000.002,10
expre=a*(x*x)+b*x+c
tabla.append([""Primera Formula "","""",""""])
lit=b*b-4*a*c
x1=(-b+math.pow(lit ,0.5))/2*a
Ev= math.fabs(x1real-x1)
Et=(Ev/x1)*100
tabla.append([x1real,x1,Et])
x2=round((-b-math.pow(lit,0.5))/2*a,5 )
Ev= math.fabs(x2real-x2)
Et=(Ev/x2)*100
tabla.append([x2real,x2,Et])
tabla.append([""Segunda Formula "","""",""""])
xx1=round((-2*c)/(b+math.pow(lit,0.5)),5)
Ev= x1real-xx1
Et=(Ev/xx1)*100
tabla.append([x1real,xx1,Et])
xx2=round((-2*c)/(b-math.pow(lit,0.5)),5)
Ev= x2real-xx2
Et=(Ev/xx2)*100
tabla.append([x2real,xx2,Et])"
Configuracion;"url='https://github.com/debimax/cours-debimax/raw/master/documents/custom.css'
with urlopen(url) as response:
    styles = response.read().decode(""utf8"")
"
Configuracion;"html_template = """"""
<script type=""text/javascript"" src=""processing.min.js""></script> 
<script type=""text/javascript"">
  var processingCode = `{0}`;
  var myCanvas = document.getElementById(""canvas`{1}`"");
  var jsCode = Processing.compile(processingCode);
  var processingInstance = new Processing(myCanvas, jsCode);
 </script>
<canvas id=""canvas`{1}`""> </canvas>    
"""""""
Configuracion;"token = mytoken
idnow=""654057261334143""
graph = facebook.GraphAPI(token)
profile = graph.get_object(id=idnow)
likes = graph.get_connections(idnow, ""likes"")

like_ids = [like['id'] for like in likes['data']]
like_names= [like['name'] for like in likes['data']]
df1=pd.DataFrame(list(zip(like_ids,like_names)))
df1.columns=(""TargetID"",""TargetName"")
df1[""SourceID""]=idnow
df1[""SourceName""]=""Bundeswehr""
df1"
Configuracion;df=pd.merge(left=df2, right=df1, left_on='TargetID', right_on='TargetID', how='left', suffixes=['','_0'])
Configuracion;"x={
  ""data"": [
    {
      ""name"": ""Bundeswehr"",
      ""id"": ""122840837780517""
    },
    {
      ""name"": ""Bundeswehr"",
      ""id"": ""518147268231689""
    },
    {
      ""name"": ""Bundeswehr"",
      ""id"": ""116477405044872""
    },
    {
      ""name"": ""Bundeswehr Wir.Dienen.Deutschland"",
      ""id"": ""214713535365449""
    },
    {
      ""name"": ""Bundeswehr Karriere"",
      ""id"": ""215977868441680""
    },
    {
      ""name"": ""Die Bundeswehr in Bayern"",
      ""id"": ""1520018021643993""
    },
    {
      ""name"": ""Die Bundeswehr in Schleswig-Holstein"",
      ""id"": ""585997238220800""
    },
    {
      ""name"": ""Deutscher BundeswehrVerband (DBwV)"",
      ""id"": ""155972661134170""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Hamburg"",
      ""id"": ""108522982545311""
    },
    {
      ""name"": ""Milit�rhistorisches Museum der Bundeswehr - MHM Dresden"",
      ""id"": ""220212508031021""
    },
    {
      ""name"": ""F�hrungsakademie der Bundeswehr"",
      ""id"": ""107782152578369""
    },
    {
      ""name"": ""Die Bundeswehr in Mecklenburg-Vorpommern"",
      ""id"": ""571802776358319""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Berlin"",
      ""id"": ""162431180449740""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Ulm"",
      ""id"": ""114199045306653""
    },
    {
      ""name"": ""Bundeswehrkrankenhaus Westerstede"",
      ""id"": ""108769105852886""
    },
    {
      ""name"": ""Info/Bundeswehr 2.0"",
      ""id"": ""580108145380270""
    },
    {
      ""name"": ""Die Bundeswehr in Th�ringen"",
      ""id"": ""153648541731952""
    },
    {
      ""name"": ""Bundeswehr und Freizeitshop"",
      ""id"": ""111189415577035""
    },
    {
      ""name"": ""Bundeswehrzentralkrankenhaus Koblenz"",
      ""id"": ""164514060226583""
    },
    {
      ""name"": ""Bundeswehrsteuererkl�rung"",
      ""id"": ""1109333195781095""
    },
    {
      ""name"": ""Universit�t der Bundeswehr M�nchen"",
      ""id"": ""167548988935""
    },
    {
      ""name"": ""Die Bundeswehr in Hamburg"",
      ""id"": ""898884626873778""
    },
    {
      ""name"": ""Schule f�r Feldj�ger und Stabsdienst der Bundeswehr Emmich-Cambrai-Kaserne"",
      ""id"": ""126011754146247""
    },
    {
      ""name"": ""Bw-K - Bundeswehr-Kameradschaft"",
      ""id"": ""160772063980987""
    },
    {
      ""name"": ""Milit�rhistorisches Museum der Bundeswehr - Flugplatz Berlin Gatow"",
      ""id"": ""141375795960016""
    }
  ],
  ""paging"": {
    ""cursors"": {
      ""before"": ""MAZDZD"",
      ""after"": ""MjQZD""
    },
    ""next"": ""https://graph.facebook.com/v2.9/search?access_token=EAACEdEose0cBANYkCCTTCeaPD60yfW8Rw0RQC0oIJJMhaetDlkQyz5ZCL4DHR1040AZBJDwLf57m6fThoRUlyMZAC8ecU9k9OOX9RAIlwCpxVoaL4TIZAaEYV9qBPyZArxRjOIxuokhux3FhMXKG166s0dfsnsQn9UGHIZAER0I1naBqZAUYMvdYy26Ahzmorq6qXBFJoNmlQZDZD&pretty=0&q=Bundeswehr&type=page&limit=25&after=MjQZD""
  }
}"
Configuracion;"t = sp.Symbol('t')
np = 2
nq = 2
n = np + nq
pp = st.symb_vector(""p1:{0}"".format(np+1))
qq = st.symb_vector(""q1:{0}"".format(nq+1))
aa = st.symb_vector(""a1:{0}"".format(nq+1))
ww = st.symb_vector(""w1:{0}"".format(nq+1))"
Configuracion;"L11, L12 = st.sorted_eigenvalues(h1)
V1 = st.sorted_eigenvector_matrix(h1)
L21, L22 = st.sorted_eigenvalues(h2)
V2 = st.sorted_eigenvector_matrix(h2)"
Configuracion;"observations = { 'tuberculosis' : 'True', 'smoker' : 'False', 'bronchitis' : 'True' }
beliefs = map( str, network.forward_backward( observations ) )"
Configuracion;"acTransList = [""SB10001,1000"", ""SB10002,1200"", ""SB10003,8000"", ""SB10004,400"", ""SB10005,300"", ""SB10006,10000"", ""SB10007,500"", ""SB10008,56"", ""SB10009,30"",""SB10010,7000"", ""CR10001,7000"", ""SB10002,-10""]
acTransRDD = sc.parallelize(acTransList)
goodTransRecords = acTransRDD.filter(lambda trans: Decimal(trans.split("","")[1]) > 0).filter(lambda trans: (trans.split("","")[0]).startswith('SB') == True)    
highValueTransRecords = goodTransRecords.filter(lambda trans: Decimal(trans.split("","")[1]) > 1000)
badAmountLambda = lambda trans: Decimal(trans.split("","")[1]) <= 0
badAcNoLambda = lambda trans: (trans.split("","")[0]).startswith('SB') == False
badAmountRecords = acTransRDD.filter(badAmountLambda)
badAccountRecords = acTransRDD.filter(badAcNoLambda)
badTransRecords  = badAmountRecords.union(badAccountRecords)"
Configuracion;"data_path = '/home/jorghyq/Project/Gwyddion-Utils/test/20160425-112013_STM--312_1.Z_mtrx'
c = gwy.gwy_file_load(data_path,gwy.RUN_NONINTERACTIVE)"
Configuracion;"f = c['/0/data']
temp = f.get_data()
temp = np.array(temp)
temp2 = temp.reshape((512,512))
temp3 = temp2 * 1e9
temp3 = temp3
temp2.shape"
Configuracion;"julio_data = {
    'blue': [
    (0.0,   0.0,    1), 
    (0.333, 0.333,    1), 
    (0.627,  0.627,    0.701), 
    (0.863, 0.863,    0.164), 
    (1,   1,    0.016), 
    ],

    'green': [
    (0.0,   1,                    0),
    (0.067, 1,    0.067),
    (0.184,  0.917,    0.184),
    (0.415, 0.623,    0.415), 
    (0.713,   0.184,                    0.713), 
    (0.909, 0.027,    0.909),
    (1,  0.023,  1),
    ],

    'red': [
    (1,   0.0,                    0),
    (0.533, 0.125,    0.125),
    (0.223,  0.369,     0.369),
    (0.067, 0.596,    0.596), 
    (0.023,   0.737,                    0.737), 
    (0.023, 1,    1)]
    }"
Configuracion;"high = 255
low = 0
amin = temp2.min()
amax = temp2.max()
rng = amax - amin
test = high - (((high - low) * (amax - temp2)) / rng)"
Configuracion;"mpl.rcParams['lines.linewidth'] = 2
font = {'family' : 'sans-serif',
        'sans-serif' : 'Verdana',
        'weight' : 'medium',
        'size'   : '12'}
params1 = {
          'axes.labelsize': 12,
          'text.fontsize': 12,
          'xtick.labelsize': 12,
          'xtick.direction': 'out',
          'ytick.labelsize': 12,
          'legend.pad': 0.01,     # empty space around the legend box
          'legend.fontsize': 12,
          'legend.labelspacing':0.25,
          'font.size': 12,
          'font.style': 'normal',
          'axes.style': 'normal',
          'xtick.labelstyle': 'normal',
          }
mpl.RcParams.update(params1)
mpl.rc('font', **font)
plt.rc(""xtick"", direction=""out"")
plt.rc(""ytick"", direction=""out"")
plt.rc('legend',**{'fontsize':12})"
Configuracion;"basin_name = ['Mercer', 'Thornton', 'Issaquah']
basin_nameL = ['mercer', 'thornton', 'issaquah']
mainpath = {}
simsedfilepath = {}
simotherfilepath = {}
simTPfilepath = {}

outputfilepath = 'D:\\Box Sync\\WQ-PAPER\\Figures\\'    "
Configuracion;"SedObs = {}
MPE_Sed = {}
Sed75 = {}
FsimSedQ4 = collections.defaultdict(list)
FobsSedQ4 = collections.defaultdict(list)
MPE_SedQ4 = collections.defaultdict(list)"
Procesado;"for i, n in enumerate(basin_name): 
    for j in range(len(FsimdateJunk1[n])):
        FsimdateJunk2[n].append(str(FsimdateJunk1[n][j][0])+'-'+str(FsimdateJunk1[n][j][1])) 
        FobsdateJunk2[n].append(str(FobsdateJunk1[n][j][0])+'-'+str(FobsdateJunk1[n][j][1]))
for i, n in enumerate(basin_name):
    for j in range(len(FsimdateJunk2[n])):
        Fsimdate[n].append(datetime.datetime.strptime(FsimdateJunk2[n][j],'%Y-%m-%d-%H:%M:%S'))
        Fobsdate[n].append(datetime.datetime.strptime(FobsdateJunk2[n][j],'%Y-%m-%d-%H:%M:%S'))
for i, n in enumerate(basin_name):
    for j in range(len(Junkdate[n])):
        FsimJunkdate[n].append(datetime.datetime.strptime(Junkdate[n][i],'%m/%d/%Y-%H:%M:%S'))"
Procesado;"for i, n in enumerate(basin_name):
    ax = plt.subplot(3, 2, jj)
    fig.subplots_adjust(hspace = 0.3, wspace = 0.3)
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    ax.text(.95,.02, str(n), fontsize=12, horizontalalignment='right', transform=ax.transAxes)
    
    textstr1 = '$R^2 = %.2f$' % r2_sed[n]
    ax.text(0.05, 0.94, textstr1, fontsize=14, transform=ax.transAxes, verticalalignment='top', bbox=props)
    
    if (n == 'Mercer'):     
        plt.xlim([0, 120])
        plt.ylim([0, 120])
        x = range(0, 121)
        y = x
        plt.plot(x, y,linestyle='--', color='grey')
    if (n == 'Thornton'): 
        plt.xlim([0, 300])
        plt.ylim([0, 300])
        # add 1:1 line
        x = range(0, 301)
        y = x
        plt.plot(x, y,linestyle='--', color='grey')
    if (n == 'Issaquah'): 
        ax.set_yscale('log')
        ax.set_xscale('log')
        x = range(0, 9000)
        y = x
        plt.plot(x, y,linestyle='--', color='grey')
        plt.xlabel('obs (mg/l))', fontsize=12., labelpad=5)
    plt.ylabel('sim (mg/l)', fontsize=12., labelpad=5)
    
    ax.scatter(sobs[n], ssim[n], s=15, c='black', alpha=0.8)    
    jj = jj + 1
    ax = plt.subplot(3, 2, jj)
    jj = jj + 1
    nnn = len(sobs[n])
    d = np.linspace(1, nnn, num=nnn)/(nnn+1)
    y  = norm.ppf(d,0,1);
    
    # create the axis ticks
    p  = [0.01, 0.25, 0.5, 0.75, 0.9, 0.99];

    tick  = norm.ppf(p,0,1);
    label = ['0.01','0.25','0.5','0.75','0.9','0.99'];
    sobs[n].sort()
    ssim[n].sort()
    ax.plot(y, ssim[n], 'r+', label='sim')
    ax.plot(y, sobs[n],'k*', label='obs')
    if (n == 'Issaquah'):
        ax.set_yscale('log')
    
    print 'Kolmogorov-Smirnov Test: ', stats.ks_2samp(ssim[n], sobs[n])
    ax.xaxis.set_major_locator(ticker.FixedLocator(tick))
    ax.xaxis.set_major_formatter(ticker.FixedFormatter(label))
    ax.tick_params(axis='x', labelsize=11)
    
    if (i == 0):
        ax.legend(loc='best', numpoints = 1)
    plt.ylabel('conc. (mg/l)', fontsize=12., labelpad=5)
    if (n == 'Issaquah'):
        plt.xlabel('probability', fontsize=12., labelpad=5)
"
Procesado;"for i in range(len(FsimSed)):
    if (FsimSed[i] > 30 or FobsSed[i] > 30):
        f.append(Fsimdate[i])
        c.append(FsimSed[i])
        d.append(FsimSedPost[i]) 
        e.append(FsimSedPre[i])"
Procesado;"for variable in list_variables:
    for categories in variable.getchildren():
        for category in categories.iter(""category""):
            for labels in category.iter(""labels""):
                for element in labels.iter(""text""):
                    if element.attrib['{http://www.w3.org/XML/1998/namespace}lang']=='de-DE':
                        print(variable.attrib['name'],category.attrib['name'], element.text)
                        df = df.append(pd.Series([variable.attrib['id'].replace('_',''),category.attrib['id'],variable.attrib['name'],category.attrib['name'], element.text]),ignore_index=True)"
Procesado;"for categories in list_categories:
    for category in categories.iter(""category""):
        for properties in category.iter(""properties""):
            for prop in properties.iter(""property""):
                try:
                    dfp = dfp.append(pd.Series([category.attrib['id'],prop.attrib['name'],prop.attrib['value']]),ignore_index=True)
                except (NameError, KeyError):
                    pass"
Procesado;" while(i<n1 and j<n2):
        if list1[i]<=list2[j]:
            merged_list.append(list1[i])
            i+=1
        else:
            merged_list.append(list2[j])
            j+=1        
    if(i==n1 and j<n2):
        for x in range(j,n2):
            merged_list.append(list2[x])
    elif(j==n2 and i<n1):
        for y in range(i,n1):
            merged_list.append(list1[y])"
Procesado;" if runNum>initialRun:
        for ii in np.arange(initialRun, runNum):
            [s,e] = parseLog(cursor, str(ii).zfill(3))
            offset += len(s)"
Procesado;"for ii,s in enumerate(start):
        if forceWrite or (ii>storedEvent+offset):
            pickleEvent(dbase, run, s, e[ii], ii+offset, path)"
Procesado;"i = 0
response_data = []
for i in range(100):
    try:
        response_data.append(requests.get(url).json())
        i += 1
        time.sleep(3)
    except ValueError:
        continue"
Procesado;"fpath = path + '/book/'
for json_file in response_data:
    with open(path + 'book_'+ str(datetime.now()) +'.json', 'w') as outfile:
        json.dump(json_file, outfile)
        time.sleep(3)"
Procesado;"book = {}
for json_file in lambda_file(json_file_path):
    with open(json_file) as json_data:
        book.update(json.load(json_data))
    print(book)"
Procesado;" for _, passenger in data.iterrows():
        if ""Sex == 'female'"":
            predictions.append(1)
        elif ""Sex == 'male'"" and 'Age'< 10 :
            predictions.append(1)
        else :
            predictions.append(0)"
Procesado;"for substance in ['Glcxt', 'Ac', 'O2']:
    ax1.plot(s['time'], s['[{}]'.format(substance)], linestyle='-', marker='s', 
             markersize=4,
             color=colors[substance], alpha=0.7, label=substance)
    ax3.plot(s['time'], s['EX_{}'.format(substance)], linestyle='-', marker=None, 
             markersize=4,
             color=colors[substance], alpha=0.7, label=substance)
    ax3.plot(s['time'], s['update_{}'.format(substance)], linestyle='-', marker='s', 
             markersize=4,
             color=colors[substance], alpha=0.7, label=substance)"
Procesado;"  for ax in (ax1, ax2, ax3, ax4):
    ax.set_xlabel('time [h]')
    ax.legend()"
Procesado;" pairs = []
    for keys,group in df[ df['target_id'] != df['user_id'] ].groupby(['user_id','target_id']):
        pairs.append( { 'source':int(keys[0]), 'target':int(keys[1]), 'total_freq':len(group) } )
    "
Procesado;" for this_time in times:
        time_str = datetime.datetime.strftime(this_time,""%Y-%m-%d %H:%M:%S"")
        for keys, group in data[data.time <= this_time].groupby(['user_id','target_id']) :
            df.ix[
               (df['source']==keys[0]) & 
               (df['target']==keys[1]), time_str
               ] = len(group)"
Procesado;"proxdata=proxdata.rename(columns = {'user.id':'user_id','remote.user.id.if.known':'target_id'})
proxdata['time'] = [datetime.datetime.strptime(stamp, ""%Y-%m-%d %H:%M:%S"") for stamp in proxdata['time']]"
Procesado;"final_proxpair_df = sum_by_period(proxpair_df, proxdata, time_array)
proxpair_df.columns = [key.split()[0] for key in proxpair_df.keys()]"
Procesado;frame = frame.append(new_line, ignore_index=True)
Procesado;"calldata=calldata.rename(columns = {'time_stamp':'time'})
comdata = pd.concat([calldata, smsdata])
comdata=comdata.rename(columns = {'dest_user_id_if_known':'target_id'})
comdata = comdata.reset_index()
comdata['time'] = [datetime.datetime.strptime(stamp, ""%Y-%m-%d %H:%M:%S"") for stamp in comdata['time']]"
Procesado;"while current_time < maxtime:
        time_array.append(current_time)
        current_time += jump"
Procesado;"  for epoch in range(training_epochs):
        total_batch = int(mnist.train.num_examples/batch_size)
        c=0.0
        avg_cost=0.0

        for i in range(total_batch):
            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)
            batch_X, batch_Y = mnist.train.next_batch(batch_size)
                _,c=sess.run([optimizer,cost], feed_dict={X:batch_X,Y_:batch_Y,lr: learning_rate})
            avg_cost += c / total_batch"
Procesado;"for m,n in matches:
        if m.distance < 0.7*n.distance:
            good.append(m)

    if len(good) > MIN_MATCH_COUNT:
        return len(good)
    else:
        return 0"
Procesado;"while not max_inlines:
        robot.angular_movement(angle, speed)
        time.sleep(1)
        image = kinect.peek_data()
        new_inlines = get_inlines(image)
        print(""new_ilneles "" , new_inlines, ""inlines: "", inlines)
        time.sleep(1)
        kinect.buffer.clear()
        clear_output(wait=True)
        
        if inlines <= new_inlines:
            inlines = new_inlines
            print(""not max_inlines"")
        else:
            max_inlines = False
            print(""yes"")
            return"
Procesado;" good = []
    for m,n in matches:
        if m.distance < 0.7*n.distance:
            good.append(m)

    if len(good) < MIN_MATCH_COUNT:
        return 0
    
    for dmatch in good:
        print(dmatch)
        point2 = kp2[dmatch[0].queryIdx].pt"
Procesado;"f = open(file_mdd,'r', encoding=""UTF-8"")
filedata = f.read()
f.close()
filecontent=filedata.replace('/Arc 3/2000-02-04','/Arc_3/2000_02_04')
filecontent=filecontent.replace('encoding=""UTF-8""','')
tree = etree.fromstring(filecontent)"
Procesado;"try:
    dfx[:0].to_sql(p_out_sql_lists,engine_dv_bbg, if_exists=p_if_exists, index=False)
except ValueError:
    pass"
Procesado;"for categoryid in list_categoryids:
    dfcat = dfcat.append(pd.Series([categoryid.attrib['value'],categoryid.attrib['name']]),ignore_index=True)
dfcat.columns=['CatValue','CatName']
dfcat.CatValue=dfcat.CatValue.astype(int)"
Procesado;"for lr, reg in sorted(results):
    train_accuracy, val_accuracy = results[(lr, reg)]
    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (
                lr, reg, train_accuracy, val_accuracy)"
Procesado;"for cls, cls_name in enumerate(classes):
    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]
    idxs = np.random.choice(idxs, examples_per_class, replace=False)
    for i, idx in enumerate(idxs):
        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)
        plt.imshow(X_test[idx].astype('uint8'))
        plt.axis('off')
        if i == 0:
            plt.title(cls_name)"
Procesado;"for i in range(20):
    if(i%2!=0):
        Exaprox=-(math.pow(x, i)/math.factorial(i))
    else:
        Exaprox=(math.pow(x, i)/math.factorial(i))
    ant=Val2
    Val2+=Exaprox
    Ever=val-Val2
    Et=(Ever/Val2)*100
    Ea=((Val2-ant)/Val2)*100
    tabla.append([i,Val2,str(Et)+"" %"",str(Ea)+"" %""])"
Procesado;"while 1:
    if(i%2==0):
        cosxaprox=math.pow(x, i)/math.factorial(i)
        ant=serie
        if(band==False):
            serie=serie+cosxaprox
            band=True
        else:
            serie=serie-cosxaprox
            band=False
        Ev=math.fabs(Val-serie)
        Et=(Ev/serie)*100
        Ea=((serie-ant)/serie)*100
        tabla.append([n,serie,str(Et)+"" %"",str(Ea)+"" %""])
        n+=1
    if(math.fabs(Ea)<Es):
        break
    i+=1"
Procesado;"with urlopen(url) as response:
    styles = response.read().decode(""utf8"")"
Procesado;"for sourceid in df1.TargetID:
    likes = graph.get_connections(sourceid, ""likes"")
    like_ids = [like['id'] for like in likes['data']]
    like_names= [like['name'] for like in likes['data']]
    dfx=pd.DataFrame(list(zip(like_ids,like_names)))
    dfx[""TargetID""]=sourceid
    df2=pd.concat([df2,dfx])   "
Procesado;"if 0:
    mod.calc_coll_part_lin_state_eq(simplify=True)
    f = mod.ff ##:
    G = mod.gg ##:
    xx = mod.x ##:
    g1 = st.col_split(G)"
Procesado;"for alpha, beta in [(1, 1), (-1, 1), (1, -1), (-1, -1)]:
    column1 = V1*Matrix([alpha*sqrt(-L12), sqrt(L11)])
    column2 = V2*Matrix([ beta*sqrt(-L22), sqrt(L21)])
    test_matrix = st.col_stack(column1, column2)"
Procesado;"for i in range(0, test_num, batch_size):
    batch = test_comment_data[perm[i:i + batch_size]]
    g_loss = generator.pretrain_step(batch)
    test_loss.append(float(g_loss.data))"
Procesado;"network.add_edge( s0, s1 )
network.add_edge( s1, s5 )
network.add_edge( s2, s3 )
network.add_edge( s2, s4 )
network.add_edge( s3, s5 )
network.add_edge( s5, s6 )
network.add_edge( s5, s7 )
network.add_edge( s4, s7 )"
Procesado;" while not (marker == 'X' or marker == 'O'):
        marker = raw_input('Player 1: Do you want to be X or O?').upper()

    if marker == 'X':
        return ('X', 'O')
    else:
        return ('O', 'X')"
Procesado;"for i in range(1,10):
        if space_check(board, i):
            return False"
Procesado;" while position not in '1 2 3 4 5 6 7 8 9'.split() or not space_check(board, int(position)):
        position = raw_input('Choose your next position: (1-9) ')"
Procesado;"while True:
    theBoard = [' '] * 10
    player1_marker, player2_marker = player_input()
    turn = choose_first()
    print(turn + ' will go first.')
    game_on = True

    while game_on:
        if turn == 'Player 1':
            display_board(theBoard)
            position = player_choice(theBoard)
            place_marker(theBoard, player1_marker, position)

            if win_check(theBoard, player1_marker):
                display_board(theBoard)
                print('Congratulations! You have won the game!')
                game_on = False
            else:
                if full_board_check(theBoard):
                    display_board(theBoard)
                    print('The game is a draw!')
                    break
                else:
                    turn = 'Player 2'

        else:
            display_board(theBoard)
            position = player_choice(theBoard)
            place_marker(theBoard, player2_marker, position)

            if win_check(theBoard, player2_marker):
                display_board(theBoard)
                print('Player 2 has won!')
                game_on = False
            else:
                if full_board_check(theBoard):
                    display_board(theBoard)
                    print('The game is a tie!')
                    break
                else:
                    turn = 'Player 1'

    if not replay():
        break"
Procesado;"for i, n in enumerate(basin_name):
    mainpath[n] = 'D:\\Dropbox\\Python_Scripts\\'+str(basin_name[i])
    simsedfilepath[n] = str(mainpath[n])+ '\\sed_sim_obs.txt'
    simotherfilepath[n] = str(mainpath[n])+'\\coliform_sim_obs.txt'
    simTPfilepath[n] = str(mainpath[n])+'\\TP_sim_obs.txt'"
Procesado;"for i, n in enumerate(basin_name): 
    FsimdateJunk1[n] = np.genfromtxt(simsedfilepath[n], dtype=str, skiprows=0, usecols=[0,1])
    FobsdateJunk1[n] = np.genfromtxt(simsedfilepath[n], dtype=str, skiprows=0, usecols=[3,4])
    FsimSed[n] = np.genfromtxt(simsedfilepath[n], dtype=float, skiprows=0, usecols=[2])
    FsimOther[n] = np.genfromtxt(simotherfilepath[n], dtype=float, skiprows=0, usecols=[2])
    FsimTP[n] = np.genfromtxt(simTPfilepath[n], dtype=float, skiprows=0, usecols=[2])
    FobsSed[n] = np.genfromtxt(simsedfilepath[n], dtype=float, skiprows=0, usecols=[5])
    FobsOther[n] = np.genfromtxt(simotherfilepath[n], dtype=float, skiprows=0, usecols=[5])
    FobsTP[n] = np.genfromtxt(simTPfilepath[n], dtype=float, skiprows=0, usecols=[5])"
Procesado;"for i, n in enumerate(basin_name): 
    for j in range(len(FsimdateJunk1[n])):
        FsimdateJunk2[n].append(str(FsimdateJunk1[n][j][0])+'-'+str(FsimdateJunk1[n][j][1])) 
        FobsdateJunk2[n].append(str(FobsdateJunk1[n][j][0])+'-'+str(FobsdateJunk1[n][j][1]))"
Procesado;"for i, n in enumerate(basin_name):
    gradient, intercept, r_value, p_value, std_err = stats.linregress(FsimSed[n],FobsSed[n])
    r2_sed[n] = r_value**2"
Procesado;"for i, n in enumerate(basin_name):
    MPE_Sed[n] = sum((x-y)/x/float(len(FobsSed[n])) for y,x in zip(FsimSed[n],FobsSed[n]))
    print str(n),"":\n"", ""%0.3f"" % MPE_Sed[n], ""(overall MPE)""
    
    SedObs[n] = pd.DataFrame(FobsSed[n])
    Sed75[n] = SedObs[n].quantile(0.75)
    
    for j in range(len(FobsSed[n])):
        if (FobsSed[n][j] > Sed75[n].values):
            idx = j
            FsimSedQ4[n].append(FsimSed[n][idx])
            FobsSedQ4[n].append(FobsSed[n][idx])
            MPE_SedQ4[n] = sum((x-y)/x/float(len(FobsSedQ4[n])) for y,x in zip(FsimSedQ4[n],FobsSedQ4[n]))"
Procesado;"for epoch in range(training_epochs):
        total_batch = int(mnist.train.num_examples/batch_size)
        c=0.0
        avg_cost=0.0
        # Loop over all batches
        for i in range(total_batch):
            
            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)
            
            batch_X, batch_Y = mnist.train.next_batch(batch_size)
            #print(batch_X.shape)
            _,c=sess.run([optimizer,cost], feed_dict={X:batch_X,Y_:batch_Y,lr: learning_rate,pkeep: 0.75})
            avg_cost += c / total_batch"
Procesado;"for i, presplit in enumerate(df[column].astype(str)):
        values = presplit.split(sep)
        if keep and len(values) > 1:
            indexes.append(i)
            new_values.append(presplit)
        for value in values:
            indexes.append(i)
            new_values.append(value)"
Procesado;"for TableName in Parent_L1_Tables.values.tolist():
    dict_levels=dict(zip(levels.DSCTableName,levels.TableName))
    TableName.replace(TableName,dict_levels[TableName])
    con = sqlite3.connect(file_ddf)
    Table=TableName.replace(TableName,dict_levels[TableName])
    df = pd.read_sql_query(""SELECT [Respondent.Serial:L] as Respondent, ""+Table+"".* from ""+Table+""  JOIN L1 ON L1.[:P0]=""+Table+"".[:P1] "", con)
    con.close()
 
    print ([Table]+df.columns.tolist())
    dfnewline=pd.DataFrame([Table]+df.columns.tolist()).T
    dfTables=pd.concat([dfTables,dfnewline])
    dfs=df[df.columns[df.columns.str.contains(':S$|Respondent|LevelId')]].copy()
    dfc1=df[df.columns[df.columns.str.contains(':C1$|Respondent|:P[12]')]].copy()
    if 'ban:S' in dfs.columns:
        dfs=dfs.drop('ban:S', axis=1)

    dict_c1=dict(zip(dfc1.columns,dfc1.columns.str.replace('(:C1|\.Serial:L)','')))

    dfc1.rename(columns=dict_c1, inplace=True)
    dfc1['Level']=TableName
    dfc1m=pd.melt(dfc1,id_vars=['Respondent','Level','LevelId',':P1'],var_name='Variable', value_name='Value')
    dfc1m['Type']='Single'

    dict_s=dict(zip(dfs.columns,dfs.columns.str.replace('(:C1|:S|\.Serial:L)','')))
    dfs.rename(columns=dict_s, inplace=True)
    dfs.loc[:,'Level']=TableName
    dfsm=pd.melt(dfs,id_vars=['Respondent','Level','LevelId'],var_name='Variable', value_name='Value')
    dfsmt=pd.DataFrame([])
    if dfsm.Respondent.count()>0:
        dfsmt=dfsm.drop(['Value'], axis=1).join(dfsm['Value'].str.split(';',expand=True).stack().reset_index(drop=True, level=1).rename('Value') )
        dfsmt=dfsmt[dfsmt.Value>' ']
        dfsmt['Value']=dfsmt['Value'].astype(int)
        dfsmt['Respondent']=dfsmt['Respondent'].astype(int)
        dfsmt['LevelId']=dfsmt['LevelId'].astype(int)
        dfsmt['Type']='Multi'

    dfL1=pd.concat([dfL1,dfc1m])
    dfL1=pd.concat([dfL1,dfsmt])"
Visualizacion;"fig = plt.figure()
ax = plt.axes()
plt.grid()
plt.show()"
Visualizacion;"plt.figure(figsize=(15,15)) 
for i in range(0,6):
    points = koch(i,TOTALWIDTH,(0,10))
    plt.subplot(2,3,i+1,aspect='equal')
    plt.plot(np.asarray(points)[:,0],np.asarray(points)[:,1],color='red')
    plt.axis('off')
plt.show()"
Visualizacion;"alphadeg = np.linspace(0.01, 22.5, 100)
alpha = np.deg2rad(alphadeg)
e = 1 - 2*(1 - np.cos(alpha))/(1 - np.cos(alpha)**2)
e = np.abs(e*100)
plt.plot(alphadeg, e)
plt.title('One-dimensional approximation error.')
plt.xlabel('$\\alpha\>\>[^\circ]$', fontsize=14)
plt.ylabel(r'$|e|\>\>[\%]$', fontsize=14)
plt.savefig('tex/figs/q1.pdf')
plt.show()"
Visualizacion;"s = nd.get_ramon(token_synapse, channel_synapse, id_synapse[3])
print s.segments[0]
vars(s)"
Visualizacion;"nx.draw_networkx(G, width=1, node_size = 100, with_labels=False, pos=nx.fruchterman_reingold_layout(G))#random_layout(G))# fruchterman_reingold_layout(G))
plt.show()
nx.draw_networkx(G, width=1, node_size = 100, with_labels=False, pos=nx.random_layout(G))#random_layout(G))
plt.show()"
Visualizacion;print '{} seconds elapsed.'.format(time.time()-start)
Visualizacion;"fig, ax = plt.subplots(2,2, figsize=(12, 7))
fig.suptitle(""Result for $q_l$"", size=16)
ax[0, 0].hist(actual_test_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[0, 0].set_title('Testing actual values')
ax[0, 1].hist(modeled_test_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[0, 1].set_title('Testing modeled values')
ax[1, 0].hist(actual_training_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[1, 0].set_title('Training actual values')
ax[1, 1].hist(modeled_training_values[:, 0], bins=np.linspace(0, 0.02, 40))
ax[1, 1].set_title('Training modeled values')
plt.show()"
Visualizacion;"df = pd.read_csv(""weather.csv"")
df"
Visualizacion;"melted = pd.melt(df, id_vars=[""day""], var_name='city', value_name='temperature')
melted"
Visualizacion;"df = fbc.cobra_reaction_info(model)
print(df)
print(""reactions:"", len(model.reactions))
print(""metabolites:"", len(model.metabolites))
print(""genes:"", len(model.genes))"
Visualizacion;"ex_idx = df.index.str.contains('^EX_')
df[ex_idx]"
Visualizacion;"zipcodes = gpd.GeoDataFrame.from_file('data/ZIP_CODE_040114/ZIP_CODE_040114.shp')
zipcodes = zipcodes.to_crs(epsg=4326)
zipcodes.columns"
Visualizacion;"for idx, diff in most_improved[:3000]:
    source_len = len(source_lines[idx].split())
    target_len = len(ref_lines[idx].split())
    edit_distance = get_editdistance(baseline_hyp_lines[idx], ref_lines[idx])
#     print(edit_distance)
    if (target_len > 10
    and target_len < 30
    and len(constraints[idx]) > 1
    and edit_distance >= 0.5):
        print(u'S1: {} S2: {}'.format(baseline_scores[idx], constrained_scores[idx]))
        print(u'Source: {}'.format(source_lines[idx]))
        print(u'Hyp1: {}'.format(baseline_hyp_lines[idx]))
        print(u'Constraints: {}'.format(constraints[idx]))
        print(u'Hyp2: {}'.format(constrained_hyp_lines[idx]))
        print(u'Ref: {}\n'.format(ref_lines[idx]))"
Visualizacion;"image = net.blobs['data'].data[4].copy()
image -= image.min()
image /= image.max()
showimage(image.transpose(1, 2, 0))"
Visualizacion;frame.shape
Visualizacion;"filters = net.params['conv1'][0].data
vis_square(filters.transpose(0, 2, 3, 1))"
Visualizacion;lucky_numbers[4]
Visualizacion;len(lucky_numbers)
Visualizacion;df.plot()
Visualizacion;df.info()
Visualizacion;df.describe()
Visualizacion;_ = df.plot(kind='scatter', x='Height', y='Weight')
Visualizacion;"df['Gendercolor'] = df['Gender'].map({'Male': 'blue', 'Female': 'red'})
df.head()"
Visualizacion;"df.plot(kind='scatter', 
        x='Height',
        y='Weight',
        c=df['Gendercolor'],
        alpha=0.3,
        title='Male & Female Populations')"
Visualizacion;"print(""labels.txt \t : \t reviews.txt\n"")
pretty_print_review_and_label(2137)
pretty_print_review_and_label(12816)
pretty_print_review_and_label(6267)
pretty_print_review_and_label(21934)
pretty_print_review_and_label(5297)
pretty_print_review_and_label(4998)"
Visualizacion;"print(""Pos-to-neg ratio for 'the' = {}"".format(pos_neg_ratios[""the""]))
print(""Pos-to-neg ratio for 'amazing' = {}"".format(pos_neg_ratios[""amazing""]))
print(""Pos-to-neg ratio for 'terrible' = {}"".format(pos_neg_ratios[""terrible""]))"
Visualizacion;"print(""Pos-to-neg ratio for 'the' = {}"".format(pos_neg_ratios[""the""]))
print(""Pos-to-neg ratio for 'amazing' = {}"".format(pos_neg_ratios[""amazing""]))
print(""Pos-to-neg ratio for 'terrible' = {}"".format(pos_neg_ratios[""terrible""]))"
Visualizacion;Image(filename='sentiment_network_2.png')
Visualizacion;"p = figure(tools=""pan,wheel_zoom,reset,save"",
           toolbar_location=""above"",
           title=""Word Positive/Negative Affinity Distribution"")
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=""#555555"")
show(p)"
Visualizacion;"p = figure(tools=""pan,wheel_zoom,reset,save"",
           toolbar_location=""above"",
           title=""The frequency distribution of the words in our corpus"")
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=""#555555"")
show(p)"
Visualizacion;Image(filename='sentiment_network_sparse.png')
Visualizacion;"p = subprocess.Popen(['C:/Program Files (x86)/biogeme-2.4/biogeme-2.4/biogeme.exe', model_name,model_data], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
for line in p.stdout.readlines():
    print (line),
"
Visualizacion;"p = subprocess.Popen(['C:/Program Files (x86)/biogeme-2.4/biogeme-2.4/biosim.exe', model_name+'_res',model_data], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
for line in p.stdout.readlines():
    print (line),
"
Visualizacion;"simres = pd.read_csv(model_name+'_res.enu',sep='\t')
simres.head()"
Visualizacion;simres.groupby('Choice_Id')['Accuracy'].mean()
Visualizacion;simres.groupby('Choice_Id')['ModeAccurate'].mean()
Visualizacion;simres.groupby(['Choice_Id','Mode_Prediction']).size()
Visualizacion;scatter(X,y)
Visualizacion;"scatter(X,y)
plot_separator(logreg.predict)"
Visualizacion;"data = np.load(""newdataset1.npz"")
sxtrain1 = data[""xtrain""]
sytrain1 = data[""ytrain""]
print(sxtrain1.shape)
print(sxtrain1.dtype)
print(sytrain1.shape)
print(sytrain1.dtype)
"
Visualizacion;"sxtrain2 = sxtrain1[:1049,:,:,:]
sytrain2 = sytrain1[:1049,:,:]
sxtest2 = sxtrain1[1049:,:,:,:]
sytest2 = sytrain1[1049:,:,:]
print(sxtrain2.shape)
print(sxtrain2.dtype)
print(sytrain2.shape)
print(sytrain2.dtype)
print(sxtest2.shape)
print(sxtest2.dtype)
print(sytest2.shape)
print(sytest2.dtype)"
Visualizacion;"xt1 = sxtrain2[0]
cimg1 = np.zeros((240,320,3),dtype=np.uint8)
cimg1[:,:,0] = xt1[0,:,:]
cimg1[:,:,1] = xt1[1,:,:]
cimg1[:,:,2] = xt1[2,:,:]

plt.imshow(cimg1)
plt.show()
plt.imshow(sytrain2[0])
plt.show()"
Visualizacion;sytrain2.dtype
Visualizacion;temp1.summary()
Visualizacion;model2.summary()
Visualizacion;"preds1 = model2.predict(sxtrain2,verbose=1)
print(preds1.shape)
plt.imshow(preds1[0])
plt.show()"
Visualizacion;"print(preds[0])
ans1 = imresize(preds[0],(480,640))
print(ans1.shape)
plt.imshow(ans1)
plt.show()"
Visualizacion;predsfine[1].shape
Visualizacion;"print(len(f1.layers))
temp12 = f1.get_layer(""convolution2d_1"")
print(f1.layers[24].output_shape)
print(f1.layers[26].output_shape)
print(f1.layers[27].output_shape)
print(f1.layers[29].output_shape)
print(f1.layers[30].output_shape)
print(f1.layers[27].output)
c1 = f1.get_layer(""lambda_2"")
print(c1.output_shape)
print(f1.layers[25].input_shape)"
Visualizacion;"print(gt1[0][63])
print(np.max(gt1[0][63]))
print(np.min(gt1[0][63]))"
